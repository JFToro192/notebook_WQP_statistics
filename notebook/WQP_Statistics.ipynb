{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Water Quality Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims at computing the descriptive statistics for the WQP maps, or any raster, by defining specific features from which these statistics are requested (located within the [in](./in) folder). In addition, the analysis of the WQP maps will consider de extraction of sampling data inside the pixels to review the estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Styling notebook\n",
    "\n",
    "# System\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Import scripts libraries for the project\n",
    "sys.path.append('./src/python')\n",
    "\n",
    "# Import the function to update the notebook style\n",
    "from nbConfig import (css_styling)\n",
    "\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Spatial Data\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterstats import zonal_stats\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from rasterio.plot import show_hist\n",
    "\n",
    "# Import custom libraries\n",
    "import wqpFunctions as wqp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the working directory for the WQP processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current Working Directories\n",
    "cwd = {\n",
    "    'local': '.',\n",
    "    'in': './in/data/S3',\n",
    "    'out': './out/data/S3',\n",
    "    'vector': './vector',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the vector files for data extraction of the WQP maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer dataset\n",
    "gdf_lakes = gpd.read_file(os.path.join(cwd['vector'],'simile_laghi','simile_laghi.shp'))\n",
    "# Buoy position in the lake\n",
    "gdf_buoy = gpd.read_file(os.path.join(cwd['vector'],'boa_sample_points','boa_sample_points.shp'))\n",
    "# Random sampling points\n",
    "gdf_sample = gpd.read_file(os.path.join(cwd['vector'],'random_points','random_points.shp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "gdf_lakes.plot(ax = ax, color= 'blue', edgecolor='k', alpha = 0.3)\n",
    "gdf_sample.plot(ax = ax, color='red')\n",
    "gdf_buoy.plot(ax = ax, color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the contents of the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(cwd['in'])\n",
    "for root, dirs, files in os.walk(cwd['in']):\n",
    "    \n",
    "    for file in files:\n",
    "        if file.endswith('.tif'):\n",
    "            print(file)\n",
    "            f = os.path.join(cwd['in'],file)\n",
    "            \n",
    "            # Read file\n",
    "            src = wqp.wqp(f)\n",
    "            src.readWQP()\n",
    "            src.name\n",
    "            \n",
    "            # Export dataset (check no data values)\n",
    "#             t = src.image.read(1)\n",
    "#             t = np.nan_to_num(t)\n",
    "#             t[t<=0] = np.nan\n",
    "#             src.writeWQP(cwd['out'],t)\n",
    "            \n",
    "            # Extract information from sampling points\n",
    "            src.extractSamplePoints(gdf_sample)\n",
    "#             src.extractSamplePoints(gdf_buoy)\n",
    "            src.samplePoint.to_csv(os.path.join(cwd['out'],'sample_points',src.name+'.csv'))\n",
    "            \n",
    "            # Compute statistics for the lakes polygons\n",
    "            src.computeStatistics(gdf_lakes, 'Nome',\"count min mean max median std  percentile_25 percentile_50 percentile_75\",0)\n",
    "            \n",
    "            # Format output\n",
    "            df = wqp.wqp.exportWQPFormatEstimates(src)\n",
    "            \n",
    "            # Export the statistics result to a file (append data if existing)\n",
    "            out_file = os.path.join(cwd['out'],'lakesStats_filtered.csv')\n",
    "            if os.path.exists(out_file):\n",
    "                df.to_csv(os.path.join(cwd['out'],'lakesStats_filtered.csv'),mode='a', header=False)\n",
    "            else:\n",
    "                df.to_csv(os.path.join(cwd['out'],'lakesStats_filtered.csv')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 'S3A_CHL_IT_20200101T100917_L1.csv'\n",
    "k = ['id',n.split('_')[1]]\n",
    "df_1 = pd.read_csv(os.path.join(cwd['out'],'sample_points','wqp_filtered',n))\n",
    "df_1 = df_1[k]\n",
    "list_names = ['dismiss','S3A_CHL_IT_20200101T100917_L1'] \n",
    "for root, dirs, files in os.walk(os.path.join(cwd['out'],'sample_points','wqp_filtered')):\n",
    "    for f in files[1:]:\n",
    "        if f.endswith('.csv'):\n",
    "            print(f)\n",
    "            df_2= pd.read_csv(os.path.join(cwd['out'],'sample_points','wqp_filtered',f))\n",
    "            k = ['id',f.split('_')[1]]\n",
    "            df_2 = df_2[k]\n",
    "            df_1 = pd.merge(df_1, df_2,  how='left', left_on=['id'], right_on = ['id'])\n",
    "            list_names.append(f.split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_1.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['names'] = list_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[0,'names']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(cwd['out'],'sample_points','wqp_tsm_filtered.csv'), encoding='utf-8', decimal='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_buoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
